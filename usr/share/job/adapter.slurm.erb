# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░
#  >>>> FLIGHT JOB ADAPTER
# ░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░░

#=================================
#  Standard Flight Job variables
#---------------------------------
# Standard environment variables.  When the job script is submitted through
# Flight Job, these variables will be set by Flight Job.  If not set, they
# default to sensible variables set by the scheduler.
FLIGHT_JOB_NAME="${FLIGHT_JOB_NAME:-$SLURM_JOB_NAME}"
FLIGHT_JOB_ID="${FLIGHT_JOB_ID:-${SLURM_ARRAY_JOB_ID:-$SLURM_JOB_ID}}"

#======================
#  Controls directory
#----------------------
# The controls directory is used internally to track various flight-job data
# files.  This is used to store the desktop session ID of the interactive
# session.
if [ "${CONTROLS_DIR}" != "" ]; then
  mkdir -p "${CONTROLS_DIR}"
fi

register_control() {
  local name
  local value
  name="$1"
  if [[ -p /dev/stdin ]]; then
      value="$( cat - )"
  else
      value="$2"
  fi
  if [ -d "${CONTROLS_DIR}" ]; then
    echo "${value}" > "${CONTROLS_DIR}/${name}"
  else
    echo "CONTROLS_DIR has not been set or is not a directory" >&2
    echo "${name} ${value}"
  fi
}

<% if template.tag('script:type') == 'interactive' -%>
<% if template.tag('session:order') == 'desktop:alloc' -%>
#=========================
#  Session orchestration
#-------------------------
# Functions used to orchestrate the creation of your interactive session on a
# login node with X11 forwarding to a compute node.

tee_session_output() {
    if [ "${SESSION_OUTPUT}" == "" ] ; then
        SESSION_OUTPUT="${RESULTS_DIR}/session.output"
    fi
    tee -a "${SESSION_OUTPUT}"
}

on_login_host() {
    [ -z $SLURM_JOBID ]
}

register_scheduler_id() {
    stdbuf -oL sed -n 's/.*job \([0-9]*\) queued.*/\1/p' \
        | head -n 1 \
        | register_control "scheduler_id" 
}

# This function is ran twice.
#
# The first time it runs, it is on the login node, where it 
#
# 1. performs X11 forwarding preparation
# 2. performs Flight Job bookkeeping
# 3. requests an allocation from the scheduler such that this script (and
#    hence this function) will be executed on the allocated node.
# 4. `exit`s out of the entire job script thus preventing the job commands
#    from running on the login node.
#
# The second time it runs, it is on the compute node, where it
#
# 1. performs Flight Job bookkeeping
# 2. completes the X11 forwarding configuration
# 3. `return`s from the function, allowing the job commands to run on the
#    compute node.
#
# This might seem like a complicated way of doing things but it greatly
# reduces the complexity of the job workload presented to the job script
# author, and keeps a high level of consistency between the job scripts
# workloads.
request_allocation() {
    if on_login_host ; then
        local submit_status
        declare -a scheduler_args
        declare -a job_script_args
        declare arg_type=scheduler_args

        # Extract scheduler arguments from job script argument.
        for i in "$@" ; do
            case "$i" in
                "--")
                    arg_type=job_script_args
                    ;;
                *)
                    if [ "${arg_type}" == "scheduler_args" ] ; then
                        scheduler_args+=($i)
                    else
                        job_script_args+=($i)
                    fi
                    ;;
            esac
        done

        # Prepare X11 forwarding
        # Add our short hostname to the DISPLAY variable if we need to
        if [ `echo $DISPLAY | cut -d: -f1 | egrep -c [a-z]` -lt 1 ] ; then
            export DISPLAYNAME=`hostname -s`$DISPLAY
        else
            export DISPLAYNAME=$DISPLAY
        fi

        # Some Flight Job bookkeeping to allow the job to be monitored /
        # controlled.
        register_control "job_type" "BOOTSTRAPPING"
        register_control "submit_status" "0"

        # Long-winded construction of `srun` arguments to ensure that we don't
        # lose any quoting along the way.
        declare -a srun_args
        srun_args+=("--pty")
        for i in "${scheduler_args[@]}" ; do
            srun_args+=("$i")
        done
        srun_args+=("${BASH_SOURCE[0]}")
        srun_args+=("$DISPLAYNAME")
        for i in "${job_script_args[@]}" ; do
            srun_args+=("$i")
        done

        # Request an allocation from the scheduler.  The allocation will run
        # this script again on the allocated node.
        echo "srun ${srun_args[@]}" | tee_session_output
        srun "${srun_args[@]}" 2> >( tee >( register_scheduler_id ) | tee_session_output )
        submit_status=$?
        if [[ $submit_status -ne 0 ]] ; then
            # More Flight Job bookkeeping.  As the job wasn't successfully
            # submitted, Flight Job cannot query the scheduler to discover its
            # status.  We set its status here.
            register_control "submit_status" ${submit_status}
            register_control "job_type" "FAILED_SUBMISSION"
            register_control "submit_stderr" "srun failed to submit job"
        else
            echo "Scheduler allocation has completed. Exiting..." | tee_session_output
            sleep 2
        fi

        # Either the allocation was succesful and has now completed or it was
        # rejected.  Either way we're done with this script on the login node.
        exit ${submit_status}

    else
        # The scheduler has allocated resources and the script is running for
        # a second time on one of the allocated nodes.

        # Some Flight Job bookkeeping to allow the job to be monitored /
        # controlled.
        register_control "scheduler_id" "$SLURM_JOB_ID"
        register_control "job_type" "BOOTSTRAPPING"

        # Setup the originator to expect our X connection.
        DISPLAYNAME=$1
        DISPLAYHOST=`echo $DISPLAYNAME | cut -d: -f1`
        this_hostname=`hostname -s`
        setxhost="ssh $DISPLAYHOST 'export DISPLAY=$DISPLAYNAME && xhost $this_hostname'"
        echo -ne "Enabling $DISPLAYHOST to accept our X-connection... " | tee_session_output
        eval $setxhost | tee_session_output
        export DISPLAY=$DISPLAYNAME

        # We return here to allow the rest of the job script, i.e., the
        # actualy workload to continue.
        return 0
    fi
}

<% else -%>
#=========================
#  Session orchestration
#-------------------------
# Functions used to orchestrate the creation of your interactive session on a
# compute node with Flight Desktop.

create_session_script() {
    SESSION_SCRIPT=$( mktemp --tmpdir session-script.XXXXXX.sh )
    echo "Creating session script ${SESSION_SCRIPT}"
    export SESSION_SCRIPT
    cat > ${SESSION_SCRIPT}
    chmod +x ${SESSION_SCRIPT}
}

session_run() {
    session_start
    session_await
}

session_start() {
    if [ "${SESSION_SCRIPT}" == "" ] ; then
        echo "Session script not created. Aborting." >&2
        exit 1
    fi
    echo "Starting session..."
    (
        set -o pipefail
        ${flight_ROOT:-/opt/flight}/bin/flight desktop \
            start \
            --script ${SESSION_SCRIPT} \
            --no-override-env \
            | tee >( grep '^Identity' | cut -f2 | register_control "flight_desktop_id" )
    )

    SESSION_STARTED=$?
    register_control "flight_desktop_status" ${SESSION_STARTED}
}

session_is_active() {
    local state
    state=$(
        ${flight_ROOT:-/opt/flight}/bin/flight desktop \
            show "${SESSION_ID}" 2>/dev/null \
            | grep '^State' \
            | cut -d $'\t' -f 2
    )
    [ "${state}" == "Active" ]
}

session_await() {
    if [ ${SESSION_STARTED} -ne 0 ] ; then
        echo "Session failed to start. Aborting." >&2
        exit 1
    fi
    local flight_session_id_path
    session_id_path="${CONTROLS_DIR}"/flight_desktop_id
    SESSION_ID=$(cat "${session_id_path}")
    SESSION_ID=${SESSION_ID##[[:space:]]}
    SESSION_ID=${SESSION_ID%%[[:space:]]}
    if [ "${SESSION_ID}" == "" ] ; then
        echo "Unable to determine session ID. Aborting." >&2
        exit 1
    fi
    echo "Session ID is ${SESSION_ID}" 
    echo "Waiting for session to end..." 
    while session_is_active ; do
        sleep 60
    done
    echo "Session is no longer running."
}
<% end -%>
<% end -%>
